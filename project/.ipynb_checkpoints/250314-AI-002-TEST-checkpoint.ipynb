{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc6b606a-fca7-4bc5-9f05-413fe33b39a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 저장된 데이터를 불러오기\n",
    "final_df = pd.read_csv('data/final_df_preprocessed.csv', encoding='utf-8-sig')\n",
    "test_df = pd.read_csv('data/test_df_preprocessed.csv', encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e21ee9d0-9f39-4ca3-8c85-55ca3657de37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  persona-id  emotion-id  \\\n",
      "0              해도 없을까 화가 난다 그냥 해결 하는 나아 부담 주고 싶지도 않고           3         368   \n",
      "1  급여 깎였어 물가 오르는데 월급 자꾸 깎이니까 너무 화가 최대한 지출 억제 해야겠어...           3         368   \n",
      "2  회사 신입 들어왔는데 말투 거슬려 그런 매일 봐야 한다고 생각 하니까 스트레스 받아...           3         368   \n",
      "3  직장 막내 이유 온갖 심부름 시켜 일도 많은 정말 분하고 섭섭해 사람 솔직하게 이야...           3         368   \n",
      "4  입사 신입사원 나를 무시 하는 같아서 너무 화가 상사 먼저 인사 하지 않아서 매일 한다고           3         368   \n",
      "\n",
      "   emotion  situation  disease  text_pca_1  text_pca_2  \n",
      "0        8          5        1   -0.045439    0.059560  \n",
      "1        8          5        1    0.036776    0.019723  \n",
      "2        8          5        1   -0.196787    0.278481  \n",
      "3        8          5        1   -0.082461    0.201029  \n",
      "4        8          5        1    0.063519    0.064646  \n",
      "                                                text  persona-id  emotion-id  \\\n",
      "0  이번 프로젝트에서 발표를 하는데 내가 실수하는 바람에 우리 팀이 감점을 받았어 너무...           2         381   \n",
      "1  회사에서 중요한 프로젝트를 혼자 하게 됐는데 솔직히 두렵고 무서워 나에게 너무 크게...           2         381   \n",
      "2  상사가 너무 무섭게 생겨서 친해지는 게 너무 두려워 무섭게 생겼는데도 업무를 보려면...           2         381   \n",
      "3  이번에 힘들게 들어간 첫 직장이거든 첫 직장이라서 그런지 너무 긴장된다 첫 직장이어...           2         381   \n",
      "4  직장에서 동료들이랑 관계가 안 좋아질까 봐 걱정돼 내가 낯가림이 심해서 친해질 수 ...           2         381   \n",
      "\n",
      "   emotion  situation  disease  text_pca_1  text_pca_2  \n",
      "0       21          5        1   -0.001888   -0.313635  \n",
      "1       21          5        1    0.011658   -0.092545  \n",
      "2       21          5        1    0.147893   -0.197571  \n",
      "3       21          5        1   -0.006208   -0.136605  \n",
      "4       21          5        1   -0.133726    0.267530  \n"
     ]
    }
   ],
   "source": [
    "print(final_df.head())\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a00eaec0-cb85-437f-baa8-13abc987fb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 감정 라벨 인코딩\n",
    "emotion_encoder = LabelEncoder()\n",
    "final_df[\"emotion\"] = emotion_encoder.fit_transform(final_df[\"emotion\"])\n",
    "test_df[\"emotion\"] = emotion_encoder.transform(test_df[\"emotion\"])\n",
    "\n",
    "# 상황 라벨 인코딩\n",
    "situation_encoder = LabelEncoder()\n",
    "final_df[\"situation\"] = situation_encoder.fit_transform(final_df[\"situation\"])\n",
    "test_df[\"situation\"] = situation_encoder.transform(test_df[\"situation\"])\n",
    "\n",
    "# 질병 라벨 인코딩\n",
    "disease_encoder = LabelEncoder()\n",
    "final_df[\"disease\"] = disease_encoder.fit_transform(final_df[\"disease\"])\n",
    "test_df[\"disease\"] = disease_encoder.transform(test_df[\"disease\"])\n",
    "\n",
    "# 레이블 인코딩: persona-id, emotion-id 숫자로 변환\n",
    "persona_encoder = LabelEncoder()\n",
    "final_df['persona-id'] = persona_encoder.fit_transform(final_df['persona-id'])\n",
    "test_df['persona-id'] = persona_encoder.transform(test_df['persona-id'])\n",
    "\n",
    "emotionid_encoder = LabelEncoder()\n",
    "final_df['emotion-id'] = emotionid_encoder.fit_transform(final_df['emotion-id'])\n",
    "test_df['emotion-id'] = emotionid_encoder.transform(test_df['emotion-id'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e2e5265-f328-4797-b3fe-30e334ef6c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  persona-id  emotion-id  \\\n",
      "0              해도 없을까 화가 난다 그냥 해결 하는 나아 부담 주고 싶지도 않고           3         368   \n",
      "1  급여 깎였어 물가 오르는데 월급 자꾸 깎이니까 너무 화가 최대한 지출 억제 해야겠어...           3         368   \n",
      "2  회사 신입 들어왔는데 말투 거슬려 그런 매일 봐야 한다고 생각 하니까 스트레스 받아...           3         368   \n",
      "3  직장 막내 이유 온갖 심부름 시켜 일도 많은 정말 분하고 섭섭해 사람 솔직하게 이야...           3         368   \n",
      "4  입사 신입사원 나를 무시 하는 같아서 너무 화가 상사 먼저 인사 하지 않아서 매일 한다고           3         368   \n",
      "\n",
      "   emotion  situation  disease  text_pca_1  text_pca_2  \n",
      "0        8          5        1   -0.045439    0.059560  \n",
      "1        8          5        1    0.036776    0.019723  \n",
      "2        8          5        1   -0.196787    0.278481  \n",
      "3        8          5        1   -0.082461    0.201029  \n",
      "4        8          5        1    0.063519    0.064646  \n",
      "                                                text  persona-id  emotion-id  \\\n",
      "0  이번 프로젝트에서 발표를 하는데 내가 실수하는 바람에 우리 팀이 감점을 받았어 너무...           2         381   \n",
      "1  회사에서 중요한 프로젝트를 혼자 하게 됐는데 솔직히 두렵고 무서워 나에게 너무 크게...           2         381   \n",
      "2  상사가 너무 무섭게 생겨서 친해지는 게 너무 두려워 무섭게 생겼는데도 업무를 보려면...           2         381   \n",
      "3  이번에 힘들게 들어간 첫 직장이거든 첫 직장이라서 그런지 너무 긴장된다 첫 직장이어...           2         381   \n",
      "4  직장에서 동료들이랑 관계가 안 좋아질까 봐 걱정돼 내가 낯가림이 심해서 친해질 수 ...           2         381   \n",
      "\n",
      "   emotion  situation  disease  text_pca_1  text_pca_2  \n",
      "0       21          5        1   -0.001888   -0.313635  \n",
      "1       21          5        1    0.011658   -0.092545  \n",
      "2       21          5        1    0.147893   -0.197571  \n",
      "3       21          5        1   -0.006208   -0.136605  \n",
      "4       21          5        1   -0.133726    0.267530  \n"
     ]
    }
   ],
   "source": [
    "print(final_df.head())\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "120f4c93-6ce8-423c-9e03-84961f20aab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 🔹 Step 1: 텍스트 데이터 벡터화 (TF-IDF)\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=100)  # 최대 100개 특성 추출\n",
    "\n",
    "# final_df에서 TF-IDF 벡터화\n",
    "tfidf_matrix_train = tfidf_vectorizer.fit_transform(final_df['text'])\n",
    "\n",
    "# test_df에서 TF-IDF 벡터화 (학습된 vectorizer 사용)\n",
    "tfidf_matrix_test = tfidf_vectorizer.transform(test_df['text'])\n",
    "\n",
    "# 🔹 Step 2: 텍스트 데이터 차원 축소 (PCA)\n",
    "pca = PCA(n_components=2)  # 2D로 축소\n",
    "\n",
    "# final_df에서 PCA 적용\n",
    "text_pca_train = pca.fit_transform(tfidf_matrix_train.toarray())\n",
    "\n",
    "# test_df에서 PCA 적용 (학습된 PCA 모델 사용)\n",
    "text_pca_test = pca.transform(tfidf_matrix_test.toarray())\n",
    "\n",
    "# 🔹 Step 3: 텍스트 벡터화된 데이터프레임에 추가\n",
    "final_df['text_pca_1'] = text_pca_train[:, 0]\n",
    "final_df['text_pca_2'] = text_pca_train[:, 1]\n",
    "\n",
    "test_df['text_pca_1'] = text_pca_test[:, 0]\n",
    "test_df['text_pca_2'] = text_pca_test[:, 1]\n",
    "\n",
    "# 🔹 Step 4: 상관 행렬 계산\n",
    "# 수치형 변수에 대해서만 상관 관계를 계산합니다.\n",
    "correlation_matrix_train = final_df[['text_pca_1', 'text_pca_2', 'emotion', 'situation', 'persona-id', 'emotion-id']].apply(pd.to_numeric, errors='coerce').corr()\n",
    "\n",
    "# # 🔹 Step 5: 상관 행렬 시각화 (final_df)\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.heatmap(correlation_matrix_train, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "# plt.title('Correlation Matrix: Text, Persona-id, Emotion-id, Emotion, Situation (Train Data)')\n",
    "# plt.show()\n",
    "\n",
    "# # 🔹 Step 6: 상황에 따른 감정 분포 (final_df)\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# sns.countplot(data=final_df, x='situation', hue='emotion', palette='Set2')\n",
    "# plt.title('Emotion Distribution per Situation (Train Data)')\n",
    "# plt.xlabel('Situation')\n",
    "# plt.ylabel('Count')\n",
    "# plt.xticks(rotation=90)\n",
    "# plt.legend(title='Emotion')\n",
    "# plt.show()\n",
    "\n",
    "# # 🔹 Step 7: 페르소나 아이디와 감정 아이디에 따른 감정 분포 (final_df)\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# sns.countplot(data=final_df, x='persona-id', hue='emotion', palette='Set1')\n",
    "# plt.title('Emotion Distribution by Persona-id (Train Data)')\n",
    "# plt.xlabel('Persona-id')\n",
    "# plt.ylabel('Count')\n",
    "# plt.legend(title='Emotion')\n",
    "# plt.xticks(rotation=90)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cb762c-962d-4ac0-8d85-265c16cdec6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636db661-7745-4661-9af9-3f829be76f09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d20ef82-5d5b-4fc0-bc6e-c137556264ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCustomDataset\u001b[39;00m(Dataset):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataframe, tokenizer, max_len, pca_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataframe \u001b[38;5;241m=\u001b[39m dataframe\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len, pca_features=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.pca_features = pca_features  # PCA 특성 (선택 사항)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.dataframe.iloc[index]['text']\n",
    "        emotion = self.dataframe.iloc[index]['emotion']\n",
    "        situation = self.dataframe.iloc[index]['situation']\n",
    "        persona_id = self.dataframe.iloc[index]['persona-id']\n",
    "        emotion_id = self.dataframe.iloc[index]['emotion-id']\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = encoding['input_ids'].squeeze(0)  # (1, seq_len) -> (seq_len,)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)  # (1, seq_len) -> (seq_len,)\n",
    "        token_type_ids = encoding['token_type_ids'].squeeze(0) if 'token_type_ids' in encoding else torch.zeros_like(input_ids)\n",
    "\n",
    "        # pca_features를 포함하도록 수정\n",
    "        pca_feature = self.pca_features[index] if self.pca_features is not None else torch.zeros(2)  # 2D PCA 특성 예시 (필요한 특성에 맞게 수정)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'token_type_ids': token_type_ids,\n",
    "            'emotion': torch.tensor(emotion, dtype=torch.long),\n",
    "            'situation': torch.tensor(situation, dtype=torch.long),\n",
    "            'persona-id': torch.tensor(persona_id, dtype=torch.long),\n",
    "            'emotion-id': torch.tensor(emotion_id, dtype=torch.long),\n",
    "            'pca_features': pca_feature  # pca_features 추가\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f311c829-0d1b-40b1-b6f4-bc699ed170ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA 적용 (예시)\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=100)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(final_df['text'])\n",
    "\n",
    "# PCA 적용 (2D로 축소)\n",
    "pca = PCA(n_components=2)\n",
    "pca_features_train = pca.fit_transform(tfidf_matrix.toarray())\n",
    "\n",
    "# 데이터셋에 PCA 특성 추가\n",
    "train_dataset = CustomDataset(final_df, tokenizer, max_len=128, pca_features=pca_features_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# PCA 특성을 test 데이터에도 적용\n",
    "tfidf_matrix_test = tfidf_vectorizer.transform(test_df['text'])\n",
    "pca_features_test = pca.transform(tfidf_matrix_test.toarray())\n",
    "\n",
    "test_dataset = CustomDataset(test_df, tokenizer, max_len=128, pca_features=pca_features_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d2f4c4-166b-4abc-a801-e46e0bb6c6d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a80b6e81-59bf-4dbd-96b4-8deebe58dbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class KoBERTMultiOutputWithEmbedding(nn.Module):\n",
    "    def __init__(self, model_name, num_emotions, num_situations, num_personas, num_emotion_ids, embedding_dim=10):\n",
    "        super(KoBERTMultiOutputWithEmbedding, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)  # KoBERT 기본 모델\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # 감정 분류 레이어 (입력 크기 확인)\n",
    "        self.emotion_classifier = nn.Linear(768 + 2 * embedding_dim, num_emotions)  # BERT 출력 + 임베딩 차원 크기\n",
    "        self.situation_classifier = nn.Linear(768, num_situations)  # 상황 예측은 BERT만 사용\n",
    "\n",
    "        # persona-id와 emotion-id를 위한 임베딩 레이어\n",
    "        self.persona_embedding = nn.Embedding(num_personas, embedding_dim)  # persona-id 임베딩\n",
    "        self.emotion_id_embedding = nn.Embedding(num_emotion_ids, embedding_dim)  # emotion-id 임베딩\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, persona_id, emotion_id, pca_features):\n",
    "        # BERT 모델에서 출력값 얻기\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooled_output = outputs.pooler_output  # (batch_size, 768)\n",
    "\n",
    "        # persona-id와 emotion-id 임베딩\n",
    "        persona_embedding = self.persona_embedding(persona_id)  # (batch_size, embedding_dim)\n",
    "        emotion_id_embedding = self.emotion_id_embedding(emotion_id)  # (batch_size, embedding_dim)\n",
    "\n",
    "        # 임베딩 벡터를 BERT 출력과 결합\n",
    "        combined_output = torch.cat((pooled_output, persona_embedding, emotion_id_embedding), dim=1)  # (batch_size, 788)\n",
    "\n",
    "        # 감정 및 상황 예측\n",
    "        emotion_logits = self.emotion_classifier(combined_output)\n",
    "        situation_logits = self.situation_classifier(pooled_output)\n",
    "\n",
    "        return emotion_logits, situation_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7da63dc0-d501-4341-b268-75a8b1cf4a1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 8, got 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 85\u001b[0m\n\u001b[0;32m     82\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# 평가\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m test_loss, emotion_accuracy, situation_accuracy, emotion_f1, situation_f1, emotion_exact_match, situation_exact_match \u001b[38;5;241m=\u001b[39m evaluate_model(\n\u001b[0;32m     86\u001b[0m     model, test_dataloader, nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(), device, emotion_encoder, situation_encoder\n\u001b[0;32m     87\u001b[0m )\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# 결과 출력\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Test Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[53], line 19\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, test_dataloader, loss_fn, device, emotion_encoder, situation_encoder)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m test_dataloader:\n\u001b[1;32m---> 19\u001b[0m         input_ids, attention_mask, token_type_ids, persona_id, emotion_id, pca_features, emotion_labels, situation_labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m     20\u001b[0m         input_ids \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     21\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 8, got 7)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def evaluate_model(model, test_dataloader, loss_fn, device, emotion_encoder, situation_encoder):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_emotion = 0\n",
    "    correct_situation = 0\n",
    "    total_emotion = 0\n",
    "    total_situation = 0\n",
    "    all_emotion_preds = []\n",
    "    all_situation_preds = []\n",
    "    all_emotion_labels = []\n",
    "    all_situation_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input_ids, attention_mask, token_type_ids, persona_id, emotion_id, pca_features, emotion_labels, situation_labels = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            token_type_ids = token_type_ids.to(device)\n",
    "            persona_id = persona_id.to(device)\n",
    "            emotion_id = emotion_id.to(device)\n",
    "            pca_features = pca_features.to(device)  # PCA feature가 있다면 사용할 수 있음\n",
    "            emotion_labels = emotion_labels.to(device)\n",
    "            situation_labels = situation_labels.to(device)\n",
    "\n",
    "            # 모델에 입력\n",
    "            emotion_logits, situation_logits = model(input_ids, attention_mask, token_type_ids, persona_id, emotion_id, pca_features)\n",
    "\n",
    "            # 손실 계산\n",
    "            loss = loss_fn(emotion_logits, emotion_labels) + loss_fn(situation_logits, situation_labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 예측값 얻기\n",
    "            emotion_pred = torch.argmax(emotion_logits, dim=1)\n",
    "            situation_pred = torch.argmax(situation_logits, dim=1)\n",
    "\n",
    "            # 정확도 계산\n",
    "            correct_emotion += (emotion_pred == emotion_labels).sum().item()\n",
    "            correct_situation += (situation_pred == situation_labels).sum().item()\n",
    "\n",
    "            total_emotion += emotion_labels.size(0)\n",
    "            total_situation += situation_labels.size(0)\n",
    "\n",
    "            # 예측값과 실제값 저장\n",
    "            all_emotion_preds.extend(emotion_pred.cpu().numpy())\n",
    "            all_situation_preds.extend(situation_pred.cpu().numpy())\n",
    "            all_emotion_labels.extend(emotion_labels.cpu().numpy())\n",
    "            all_situation_labels.extend(situation_labels.cpu().numpy())\n",
    "\n",
    "    # 평균 손실 계산\n",
    "    avg_loss = total_loss / len(test_dataloader)\n",
    "\n",
    "    # 감정과 상황 정확도 계산\n",
    "    emotion_accuracy = correct_emotion / total_emotion\n",
    "    situation_accuracy = correct_situation / total_situation\n",
    "\n",
    "    # F1-Score 계산 (감정, 상황)\n",
    "    emotion_f1 = f1_score(all_emotion_labels, all_emotion_preds, average='weighted')\n",
    "    situation_f1 = f1_score(all_situation_labels, all_situation_preds, average='weighted')\n",
    "\n",
    "    # Exact Match 계산\n",
    "    emotion_exact_match = (emotion_encoder.inverse_transform(all_emotion_labels) == emotion_encoder.inverse_transform(all_emotion_preds)).mean()\n",
    "    situation_exact_match = (situation_encoder.inverse_transform(all_situation_labels) == situation_encoder.inverse_transform(all_situation_preds)).mean()\n",
    "\n",
    "    return avg_loss, emotion_accuracy, situation_accuracy, emotion_f1, situation_f1, emotion_exact_match, situation_exact_match\n",
    "\n",
    "# 모델 초기화\n",
    "model_name = \"monologg/kobert\"  # KoBERT 모델명\n",
    "num_emotions = len(final_df[\"emotion\"].unique())  # 감정 클래스 개수\n",
    "num_situations = len(final_df[\"situation\"].unique())  # 상황 클래스 개수\n",
    "model = KoBERTMultiOutputWithEmbedding(model_name, num_emotions, num_situations, num_personas, num_emotion_ids)\n",
    "\n",
    "# device 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# best_model.pth 로드\n",
    "model.load_state_dict(torch.load(\"kobert_emotion_situation/best_model.pth\"))\n",
    "model.to(device)\n",
    "\n",
    "# 평가\n",
    "test_loss, emotion_accuracy, situation_accuracy, emotion_f1, situation_f1, emotion_exact_match, situation_exact_match = evaluate_model(\n",
    "    model, test_dataloader, nn.CrossEntropyLoss(), device, emotion_encoder, situation_encoder\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"✅ Test Loss: {test_loss:.4f}\")\n",
    "print(f\"✅ Emotion Accuracy: {emotion_accuracy:.4f}\")\n",
    "print(f\"✅ Situation Accuracy: {situation_accuracy:.4f}\")\n",
    "print(f\"✅ Emotion F1-Score: {emotion_f1:.4f}\")\n",
    "print(f\"✅ Situation F1-Score: {situation_f1:.4f}\")\n",
    "print(f\"✅ Emotion Exact Match: {emotion_exact_match:.4f}\")\n",
    "print(f\"✅ Situation Exact Match: {situation_exact_match:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7786500d-472d-487c-a744-307c39642ba8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "KoBERTMultiOutputWithEmbedding.__init__() missing 2 required positional arguments: 'num_personas' and 'num_emotion_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# ✅ 3. 모델 초기화\u001b[39;00m\n\u001b[0;32m     13\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonologg/kobert\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# KoBERT 모델명\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m model \u001b[38;5;241m=\u001b[39m KoBERTMultiOutputWithEmbedding(model_name, num_emotions, num_situations)  \u001b[38;5;66;03m# num_emotions, num_situations 정의 완료\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# ✅ 4. device 설정 (GPU/CPU)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# CUDA (GPU) 사용 가능 여부 확인\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: KoBERTMultiOutputWithEmbedding.__init__() missing 2 required positional arguments: 'num_personas' and 'num_emotion_ids'"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from transformers import BertModel, AutoTokenizer\n",
    "# from sklearn.metrics import accuracy_score, f1_score\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# # ✅ 2. 감정과 상황의 클래스 수 계산\n",
    "# num_emotions = len(final_df[\"emotion\"].unique())  # 감정 클래스 개수\n",
    "# num_situations = len(final_df[\"situation\"].unique())  # 상황 클래스 개수\n",
    "\n",
    "# # ✅ 3. 모델 초기화\n",
    "# model_name = \"monologg/kobert\"  # KoBERT 모델명\n",
    "# model = KoBERTMultiOutputWithEmbedding(model_name, num_emotions, num_situations)  # num_emotions, num_situations 정의 완료\n",
    "\n",
    "# # ✅ 4. device 설정 (GPU/CPU)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # CUDA (GPU) 사용 가능 여부 확인\n",
    "# model.to(device)  # 모델을 해당 device로 이동\n",
    "\n",
    "# # 모델 체크포인트 불러오기\n",
    "# model.load_state_dict(torch.load(\"kobert_emotion_situation/best_model.pth\"))\n",
    "# model.to(device)  # 다시 device로 모델 이동\n",
    "\n",
    "# # ✅ 5. 학습 기록 불러오기\n",
    "# history = torch.load(\"kobert_emotion_situation/history.pth\", weights_only=True)\n",
    "# train_loss_history = history['train_loss']\n",
    "# train_accuracy_history = history['train_accuracy']\n",
    "\n",
    "# # ✅ 6. 테스트 데이터셋 준비 (test_dataloader)\n",
    "# test_texts = test_df[\"text\"].tolist()  # 'text' 컬럼에 테스트 데이터가 있다고 가정\n",
    "# test_encodings = tokenizer(test_texts, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "# emotion_labels_test = emotion_encoder.transform(test_df['emotion'].values)  # 감정 라벨\n",
    "# situation_labels_test = situation_encoder.transform(test_df['situation'].values)  # 상황 라벨\n",
    "\n",
    "# # 텐서로 변환\n",
    "# emotion_labels_test = torch.tensor(emotion_labels_test)\n",
    "# situation_labels_test = torch.tensor(situation_labels_test)\n",
    "\n",
    "# # Test Dataset과 DataLoader 생성\n",
    "# test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_encodings['token_type_ids'],\n",
    "#                              emotion_labels_test, situation_labels_test)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "\n",
    "# # ✅ 7. 모델 평가 함수 (F1-Score와 Exact Match 추가)\n",
    "# def evaluate_model(model, test_dataloader, loss_fn, device, emotion_encoder, situation_encoder):\n",
    "#     model.eval()\n",
    "#     total_loss = 0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     all_emotion_preds = []\n",
    "#     all_situation_preds = []\n",
    "#     all_emotion_labels = []\n",
    "#     all_situation_labels = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for batch in test_dataloader:\n",
    "#             input_ids, attention_mask, token_type_ids, emotion_labels, situation_labels = batch\n",
    "#             input_ids = input_ids.to(device)\n",
    "#             attention_mask = attention_mask.to(device)\n",
    "#             token_type_ids = token_type_ids.to(device)\n",
    "#             emotion_labels = emotion_labels.to(device)\n",
    "#             situation_labels = situation_labels.to(device)\n",
    "\n",
    "#             emotion_logits, situation_logits = model(input_ids, attention_mask, token_type_ids)\n",
    "#             loss = loss_fn(emotion_logits, emotion_labels) + loss_fn(situation_logits, situation_labels)\n",
    "\n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#             emotion_pred = torch.argmax(emotion_logits, dim=1)\n",
    "#             situation_pred = torch.argmax(situation_logits, dim=1)\n",
    "\n",
    "#             correct += (emotion_pred == emotion_labels).sum().item()\n",
    "#             correct += (situation_pred == situation_labels).sum().item()\n",
    "#             total += emotion_labels.size(0) + situation_labels.size(0)\n",
    "\n",
    "#             all_emotion_preds.extend(emotion_pred.cpu().numpy())\n",
    "#             all_situation_preds.extend(situation_pred.cpu().numpy())\n",
    "#             all_emotion_labels.extend(emotion_labels.cpu().numpy())\n",
    "#             all_situation_labels.extend(situation_labels.cpu().numpy())\n",
    "\n",
    "#     avg_loss = total_loss / len(test_dataloader)\n",
    "#     accuracy = correct / total\n",
    "\n",
    "#     # 감정과 상황에 대해 F1-Score 계산\n",
    "#     emotion_f1 = f1_score(all_emotion_labels, all_emotion_preds, average='weighted')\n",
    "#     situation_f1 = f1_score(all_situation_labels, all_situation_preds, average='weighted')\n",
    "\n",
    "#     # Exact Match 계산 (True는 1.0, False는 0.0으로 변환)\n",
    "#     # 예측값을 디코딩하여 정확히 일치하는지 확인\n",
    "#     emotion_exact_match = (emotion_encoder.inverse_transform(all_emotion_labels) == emotion_encoder.inverse_transform(all_emotion_preds)).mean()\n",
    "#     situation_exact_match = (situation_encoder.inverse_transform(all_situation_labels) == situation_encoder.inverse_transform(all_situation_preds)).mean()\n",
    "\n",
    "#     # 감정 정확도, 상황 정확도 계산\n",
    "#     emotion_accuracy = accuracy_score(all_emotion_labels, all_emotion_preds)\n",
    "#     situation_accuracy = accuracy_score(all_situation_labels, all_situation_preds)\n",
    "\n",
    "#     return avg_loss, accuracy, emotion_accuracy, situation_accuracy, emotion_f1, situation_f1, emotion_exact_match, situation_exact_match\n",
    "\n",
    "\n",
    "\n",
    "# # ✅ 8. 손실 함수 정의\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# # ✅ 9. 모델 평가 실행\n",
    "# test_loss, test_accuracy, emotion_accuracy, situation_accuracy, emotion_f1, situation_f1, emotion_exact_match, situation_exact_match = evaluate_model(\n",
    "#     model, test_dataloader, loss_fn, device, emotion_encoder, situation_encoder\n",
    "# )\n",
    "\n",
    "\n",
    "# # ✅ 10. 결과 출력\n",
    "# print(f\"✅ Test Loss: {test_loss:.4f}\")\n",
    "# print(f\"✅ Test Accuracy: {test_accuracy:.4f}\")\n",
    "# print(f\"✅ Emotion Accuracy: {emotion_accuracy:.4f}\")\n",
    "# print(f\"✅ Situation Accuracy: {situation_accuracy:.4f}\")\n",
    "# print(f\"✅ Emotion F1-Score: {emotion_f1:.4f}\")\n",
    "# print(f\"✅ Situation F1-Score: {situation_f1:.4f}\")\n",
    "# print(f\"✅ Emotion Exact Match: {emotion_exact_match:.4f}\")\n",
    "# print(f\"✅ Situation Exact Match: {situation_exact_match:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa0e1c8-769b-4c1b-9546-977f7beddaf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e335fb31-205a-42b7-b886-4b1a0a3b8002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a44901-b63a-4f57-b202-dcf683e2979e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
