{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c6fc965-ca47-46d7-a489-09e135cce68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu118\n",
      "True\n",
      "11.8\n",
      "NVIDIA GeForce RTX 3050\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())  # CUDA가 활성화 여부\n",
    "print(torch.version.cuda)  # 설치된 CUDA 버전 출력\n",
    "print(torch.cuda.get_device_name(0))  # 사용 중인 GPU\n",
    "print(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a0411ec3-f8c6-47d9-8ccd-883e0e522592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv('data/train_dataset.csv')\n",
    "test_df = pd.read_csv('data/test_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "739ed61f-7382-403e-8bb5-7c41a8ebcc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text emotion situation\n",
      "0  일은 왜 해도 해도 끝이 없을까? 화가 난다. 그냥 내가 해결하는 게 나아. 남들한...     E18       S06\n",
      "1  이번 달에 또 급여가 깎였어! 물가는 오르는데 월급만 자꾸 깎이니까 너무 화가 나....     E18       S06\n",
      "2  회사에 신입이 들어왔는데 말투가 거슬려. 그런 애를 매일 봐야 한다고 생각하니까 스...     E18       S06\n",
      "3  직장에서 막내라는 이유로 나에게만 온갖 심부름을 시켜. 일도 많은 데 정말 분하고 ...     E18       S06\n",
      "4  얼마 전 입사한 신입사원이 나를 무시하는 것 같아서 너무 화가 나. 상사인 나에게 ...     E18       S06\n",
      "                                                text emotion situation\n",
      "0  이번 프로젝트에서 발표를 하는데 내가 실수하는 바람에 우리 팀이 감점을 받았어. 너...     E31       S06\n",
      "1  회사에서 중요한 프로젝트를 혼자 하게 됐는데 솔직히 두렵고 무서워. 나에게 너무 크...     E31       S06\n",
      "2  상사가 너무 무섭게 생겨서 친해지는 게 너무 두려워. 무섭게 생겼는데도 업무를 보려...     E31       S06\n",
      "3  이번에 힘들게 들어간 첫 직장이거든. 첫 직장이라서 그런지 너무 긴장된다. 첫 직장...     E31       S06\n",
      "4  직장에서 동료들이랑 관계가 안 좋아질까 봐 걱정돼. 내가 낯가림이 심해서 친해질 수...     E31       S06\n"
     ]
    }
   ],
   "source": [
    "print(train_df.head())\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fd70f052-fbc1-498a-8dc0-92be2607d64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51628 entries, 0 to 51627\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   text       51628 non-null  object\n",
      " 1   emotion    51628 non-null  object\n",
      " 2   situation  51628 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.2+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6640 entries, 0 to 6639\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   text       6640 non-null   object\n",
      " 1   emotion    6640 non-null   object\n",
      " 2   situation  6640 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 155.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(train_df.info())\n",
    "print(test_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "12b33529-4272-4823-9563-39e0b0115f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'일은 왜 해도 해도 끝이 없을까? 화가 난다. 그냥 내가 해결하는 게 나아. 남들한테 부담 주고 싶지도 않고.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5d6c80f3-2168-4024-979c-c090c6fe4452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# 한국어 불용어 리스트 가져오기\n",
    "url = \"https://raw.githubusercontent.com/stopwords-iso/stopwords-ko/master/stopwords-ko.txt\"\n",
    "response = requests.get(url)\n",
    "korean_stopwords = set(response.text.split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "40a4b01a-a6d2-4a04-83dd-c30c6638c5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_with_adverbs_and_verbs(text):\n",
    "    # 텍스트가 비어있는 경우\n",
    "    if not isinstance(text, str):  \n",
    "        return \"\"\n",
    "    \n",
    "    parsed_text = mecab.parse(text).split(\"\\n\")[:-2]  # 마지막 줄(EOS, 공백) 제거\n",
    "    tokens = [line.split(\"\\t\") for line in parsed_text if \"\\t\" in line]\n",
    "    \n",
    "    # 품사 필터링 (명사, 형용사, 부사만 남기기)\n",
    "    keywords = []\n",
    "    for token in tokens:\n",
    "        if len(token) < 2:\n",
    "            continue\n",
    "        word, features = token[0], token[1].split(\",\")\n",
    "        tag = features[0]  \n",
    "        lemma = features[-3]  # 원형을 추출\n",
    "\n",
    "        # 명사(NNG, NNP) 그대로 추가\n",
    "        if tag in ['NNG', 'NNP']:\n",
    "            keywords.append(word)\n",
    "\n",
    "        # 형용사(VA) 원형 변환 및 추가\n",
    "        elif tag == 'VA':\n",
    "            if lemma != '*' and lemma != '' and lemma != word:\n",
    "                keywords.append(lemma)\n",
    "            else:\n",
    "                keywords.append(word + \"다\")\n",
    "\n",
    "        # 부사(MAG)는 감정의 강도나 상태를 강조하므로 추가\n",
    "        elif tag == 'MAG':  # 부사 태그\n",
    "            keywords.append(word)\n",
    "\n",
    "    # 중복 단어 제거 & 불용어 제거\n",
    "    keywords = list(dict.fromkeys(keywords))  # 중복 제거\n",
    "    keywords = [word for word in keywords if word not in stopwords and len(word) > 1]  # 불용어 제거\n",
    "    \n",
    "    return \" \".join(keywords)\n",
    "\n",
    "# MeCab 적용하여 텍스트 전처리\n",
    "train_df['text'] = train_df['text'].apply(preprocess_text_with_adverbs_and_verbs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "20e90f66-2c13-4cf4-97a5-9c1804c59466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               text emotion situation\n",
      "0                       없다 그냥 해결 부담     E18       S06\n",
      "1    급여 물가 월급 자꾸 너무 최대한 지출 억제 고정 없다     E18       S06\n",
      "2  회사 신입 말투 매일 생각 스트레스 사람 억지로 거리 예의     E18       S06\n",
      "3  직장 막내 이유 심부름 많다 정말 분하다 사람 이야기 방해     E18       S06\n",
      "4        입사 신입 사원 무시 너무 상사 먼저 인사 매일     E18       S06\n"
     ]
    }
   ],
   "source": [
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422b7c8a-247f-42e3-bcbf-95d7431b6347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2d0dbae-db21-44bf-81f2-b7e6789f55b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# print(len(train_df['emotion'].unique()), \"개\", end='\\n\\n')\n",
    "# print(train_df['emotion'].value_counts().max())\n",
    "# print(train_df['emotion'].value_counts().min())\n",
    "# print((train_df['emotion'].value_counts().max()-train_df['emotion'].value_counts().min())/60)\n",
    "# print(train_df['emotion'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d7057ba-c015-4584-9e3f-ff3c5773d330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(train_df['situation'].unique()), \"개\", end='\\n\\n')\n",
    "# print(train_df['situation'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "997a1355-b707-4494-bdb5-5f1ea028da92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Create a figure and axes for two vertical subplots\n",
    "# fig, axes = plt.subplots(2, 1, figsize=(14, 14))\n",
    "\n",
    "# # Emotion distribution bar chart\n",
    "# axes[0].bar(train_df['emotion'].value_counts().index, train_df['emotion'].value_counts().values, color='skyblue', edgecolor='black')\n",
    "# axes[0].set_title('Emotion Distribution')\n",
    "# axes[0].set_xlabel('Emotion')\n",
    "# axes[0].set_ylabel('Count')\n",
    "# axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# # Situation distribution bar chart\n",
    "# axes[1].bar(train_df['situation'].value_counts().index, train_df['situation'].value_counts().values, color='salmon', edgecolor='black')\n",
    "# axes[1].set_title('Situation Distribution')\n",
    "# axes[1].set_xlabel('Situation')\n",
    "# axes[1].set_ylabel('Count')\n",
    "# axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# plt.tight_layout(pad=4.0)  # Adjust layout and padding for better label placement\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "011d4d76-a532-43b1-b512-ccb964ac7ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^가-힣a-zA-Z0-9\\s]\", \"\", text)  # 한글, 영어, 숫자, 공백만 유지\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # 중복 공백 제거\n",
    "    return text\n",
    "\n",
    "train_df[\"text\"] = train_df[\"text\"].apply(clean_text)\n",
    "test_df[\"text\"] = test_df[\"text\"].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d3fa20b7-bbe4-47fa-9ea4-57f2b5690662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>situation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>없다 그냥 해결 부담</td>\n",
       "      <td>E18</td>\n",
       "      <td>S06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>급여 물가 월급 자꾸 너무 최대한 지출 억제 고정 없다</td>\n",
       "      <td>E18</td>\n",
       "      <td>S06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>회사 신입 말투 매일 생각 스트레스 사람 억지로 거리 예의</td>\n",
       "      <td>E18</td>\n",
       "      <td>S06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>직장 막내 이유 심부름 많다 정말 분하다 사람 이야기 방해</td>\n",
       "      <td>E18</td>\n",
       "      <td>S06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>입사 신입 사원 무시 너무 상사 먼저 인사 매일</td>\n",
       "      <td>E18</td>\n",
       "      <td>S06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               text emotion situation\n",
       "0                       없다 그냥 해결 부담     E18       S06\n",
       "1    급여 물가 월급 자꾸 너무 최대한 지출 억제 고정 없다     E18       S06\n",
       "2  회사 신입 말투 매일 생각 스트레스 사람 억지로 거리 예의     E18       S06\n",
       "3  직장 막내 이유 심부름 많다 정말 분하다 사람 이야기 방해     E18       S06\n",
       "4        입사 신입 사원 무시 너무 상사 먼저 인사 매일     E18       S06"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6139a0fb-ff88-4343-93c4-311238920b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'없다 그냥 해결 부담'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6583a56e-ed83-4570-9f15-752f7068aab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 감정 라벨 인코딩\n",
    "emotion_encoder = LabelEncoder()\n",
    "train_df[\"emotion\"] = emotion_encoder.fit_transform(train_df[\"emotion\"])\n",
    "test_df[\"emotion\"] = emotion_encoder.transform(test_df[\"emotion\"])\n",
    "\n",
    "# 상황 라벨 인코딩\n",
    "situation_encoder = LabelEncoder()\n",
    "train_df[\"situation\"] = situation_encoder.fit_transform(train_df[\"situation\"])\n",
    "test_df[\"situation\"] = situation_encoder.transform(test_df[\"situation\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dbcdf34a-3fb5-42ad-b1e8-c1fd059ba66e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>situation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>없다 그냥 해결 부담</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>급여 물가 월급 자꾸 너무 최대한 지출 억제 고정 없다</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>회사 신입 말투 매일 생각 스트레스 사람 억지로 거리 예의</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>직장 막내 이유 심부름 많다 정말 분하다 사람 이야기 방해</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>입사 신입 사원 무시 너무 상사 먼저 인사 매일</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               text  emotion  situation\n",
       "0                       없다 그냥 해결 부담        8          5\n",
       "1    급여 물가 월급 자꾸 너무 최대한 지출 억제 고정 없다        8          5\n",
       "2  회사 신입 말투 매일 생각 스트레스 사람 억지로 거리 예의        8          5\n",
       "3  직장 막내 이유 심부름 많다 정말 분하다 사람 이야기 방해        8          5\n",
       "4        입사 신입 사원 무시 너무 상사 먼저 인사 매일        8          5"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f27276bf-5b0b-4bfd-96eb-b3df44990190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, AutoTokenizer\n",
    "\n",
    "# ✅ 1. KoBERT 커스텀 모델 정의 (감정 + 상황 다중 분류)\n",
    "class KoBERTMultiOutput(nn.Module):\n",
    "    def __init__(self, model_name, num_emotions, num_situations):\n",
    "        super(KoBERTMultiOutput, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)  # KoBERT 기본 모델\n",
    "        self.dropout = nn.Dropout(0.1)  # 드롭아웃 레이어 추가\n",
    "        self.emotion_classifier = nn.Linear(self.bert.config.hidden_size, num_emotions)  # 감정 분류기\n",
    "        self.situation_classifier = nn.Linear(self.bert.config.hidden_size, num_situations)  # 상황 분류기\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        # Bert 모델을 통해 출력값을 얻음\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooled_output = self.dropout(outputs.pooler_output)  # BERT 출력에서 pooling된 벡터에 드롭아웃 적용\n",
    "\n",
    "        # 감정과 상황을 위한 로짓 분리\n",
    "        emotion_logits = self.emotion_classifier(pooled_output)\n",
    "        situation_logits = self.situation_classifier(pooled_output)\n",
    "\n",
    "        # 두 가지 로짓 반환\n",
    "        return emotion_logits, situation_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fa0329f2-5dd1-41a3-9625-92113dcfe30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[   2,  806, 5156, 1458, 1632, 1786, 6273, 6003, 7318, 3742,    3,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2, 4627, 4949, 4872,  517, 6189, 7431, 7096, 3273, 2267, 5468, 2734,\n",
      "         6542,  517, 7095, 5731,    3,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2, 1423, 4627, 4396, 7316, 1437,  950, 4206, 3273, 3756, 3343,    3,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2, 4604, 1458, 3774, 3055, 2584, 4455, 3673, 3273, 2063, 3891, 2406,\n",
      "         1951, 5156, 3220, 7774,    3,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2, 3257, 4996, 3273, 3673,  517, 5330, 5663, 1917, 3114, 1951, 4206,\n",
      "         1042, 3367, 1370, 7295, 4355, 4598, 7342, 1956,    3,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2, 3042, 4998, 5439,  972, 4212, 7408, 5341, 4847,  993, 6542, 1425,\n",
      "         2986, 1585, 5563, 3673, 3647, 5812, 3273,    3,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2, 1680, 3742, 5176, 7953,  993, 5944, 5491, 1254, 1429, 5782,  938,\n",
      "         1493, 4249, 4136, 1574,    3,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2, 1996, 4571,  984, 5101, 2474, 2705, 1927, 6527, 1549,    3,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2, 2573, 7467, 5561, 1750,  517, 5330, 5646, 2207, 7846, 2618, 4206,\n",
      "          765, 3219,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,  517, 5399, 6368, 3437, 6222,  517, 7352, 1951, 1476, 5712, 7482,\n",
      "         1002, 5382,  517, 7219, 5782,  882, 1088, 3367, 2705, 4948, 5808, 1549,\n",
      "         3273, 4229, 1717, 4919, 4083, 5213, 5782, 1174, 7396, 5782, 4722,    3,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2, 1493, 4249, 4103, 1951,  517, 6357, 3273, 3462, 2934, 3140,    3,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2, 3111, 6122, 3514, 4368, 4396, 7316,  807, 6493, 4326, 3367, 4491,\n",
      "         7295, 1192, 3742,  755, 6519, 3006, 5788, 2971,    3,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2, 4206, 4355, 3264, 6527, 3042,  517, 7219, 5782, 3742, 4102,  765,\n",
      "         1659, 2279, 1002,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2, 4299, 4684,  517, 6186, 5782, 1423,  950, 3673, 2618, 4206, 4551,\n",
      "         4223,  765, 3321,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2, 4384, 6498, 3468, 7468, 1113, 3943, 3520, 3897, 4773, 6039, 4212,\n",
      "         7408, 5341, 3891, 3533, 6165, 4102, 3273, 1058, 1076, 4206, 4534,  765,\n",
      "         2149, 6812,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2, 1039, 3005, 2433, 1282, 4206, 3039, 5341, 4684, 1961,    3,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1]]), tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), tensor([40,  5,  3, 34, 35, 40,  9, 24, 25, 49,  4,  3, 42, 16,  9, 17]), tensor([ 9,  2,  4,  5,  0, 11, 11,  5,  0,  3,  7,  8, 11,  6, 10,  5])]\n",
      "[tensor([[   2, 3697, 4902,  ...,    1,    1,    1],\n",
      "        [   2, 5156, 6903,  ...,    1,    1,    1],\n",
      "        [   2, 2658, 6494,  ...,    1,    1,    1],\n",
      "        ...,\n",
      "        [   2, 1434, 4627,  ...,    1,    1,    1],\n",
      "        [   2, 1435, 1956,  ...,    1,    1,    1],\n",
      "        [   2, 4627, 5330,  ...,    1,    1,    1]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), tensor([21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 15, 15, 15, 15]), tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 2, 2, 2])]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ✅ 1. Tokenizer 초기화\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"monologg/kobert\",  trust_remote_code=True)  # KoBERT 모델에 맞는 토크나이저 사용\n",
    "\n",
    "# ✅ 2. train_df에서 텍스트를 토큰화 (훈련 데이터)\n",
    "train_texts = train_df[\"text\"].tolist()  # 'text' 컬럼에 훈련 데이터가 있다고 가정\n",
    "train_encodings = tokenizer(train_texts, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")  # 토큰화\n",
    "\n",
    "# ✅ 3. 라벨 데이터 (감정, 상황)\n",
    "# 감정과 상황을 숫자 레이블로 변환\n",
    "emotion_encoder = LabelEncoder()\n",
    "situation_encoder = LabelEncoder()\n",
    "\n",
    "emotion_labels = emotion_encoder.fit_transform(train_df['emotion'].values)  # 감정 라벨\n",
    "situation_labels = situation_encoder.fit_transform(train_df['situation'].values)  # 상황 라벨\n",
    "\n",
    "# 텐서로 변환\n",
    "emotion_labels = torch.tensor(emotion_labels)\n",
    "situation_labels = torch.tensor(situation_labels)\n",
    "\n",
    "# ✅ 4. TensorDataset 생성 (입력 데이터와 레이블을 묶음)\n",
    "input_ids = train_encodings['input_ids']\n",
    "attention_mask = train_encodings['attention_mask']\n",
    "token_type_ids = train_encodings['token_type_ids'] if 'token_type_ids' in train_encodings else torch.zeros_like(input_ids)\n",
    "\n",
    "# TensorDataset을 이용해 훈련 데이터셋을 생성\n",
    "train_dataset = TensorDataset(input_ids, attention_mask, token_type_ids, emotion_labels, situation_labels)\n",
    "\n",
    "# ✅ 5. DataLoader 생성 (배치 크기 설정)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)  # 배치 크기 16으로 설정\n",
    "\n",
    "\n",
    "# ✅ 6. 테스트 데이터셋 생성\n",
    "test_texts = test_df[\"text\"].tolist()  # 'text' 컬럼에 테스트 데이터가 있다고 가정\n",
    "test_encodings = tokenizer(test_texts, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")  # 토큰화\n",
    "\n",
    "# 테스트 데이터 라벨 (감정, 상황)\n",
    "emotion_labels_test = emotion_encoder.transform(test_df['emotion'].values)  # 감정 라벨\n",
    "situation_labels_test = situation_encoder.transform(test_df['situation'].values)  # 상황 라벨\n",
    "\n",
    "# 텐서로 변환\n",
    "emotion_labels_test = torch.tensor(emotion_labels_test)\n",
    "situation_labels_test = torch.tensor(situation_labels_test)\n",
    "\n",
    "# ✅ 7. TensorDataset 생성 (입력 데이터와 테스트 레이블을 묶음)\n",
    "input_ids_test = test_encodings['input_ids']\n",
    "attention_mask_test = test_encodings['attention_mask']\n",
    "token_type_ids_test = test_encodings['token_type_ids'] if 'token_type_ids' in test_encodings else torch.zeros_like(input_ids_test)\n",
    "\n",
    "# TensorDataset을 이용해 테스트 데이터셋을 생성\n",
    "test_dataset = TensorDataset(input_ids_test, attention_mask_test, token_type_ids_test, emotion_labels_test, situation_labels_test)\n",
    "\n",
    "# ✅ 8. DataLoader 생성 (배치 크기 설정)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)  # 배치 크기 16으로 설정\n",
    "\n",
    "\n",
    "# ✅ 9. 배치 데이터 확인 (선택 사항)\n",
    "for batch in train_dataloader:\n",
    "    print(batch)  # 첫 번째 배치 확인\n",
    "    break  # 첫 번째 배치만 확인\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    print(batch)  # 첫 번째 테스트 배치 확인\n",
    "    break  # 첫 번째 배치만 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26ceb93-c004-405c-90ba-6b22cbf9f354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kj\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/20: 100%|████████████████████████████████████████| 3227/3227 [08:09<00:00,  6.59it/s, Loss=4.6, Accuracy=0.252]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 1/20 - Loss: 5.4871 - Accuracy: 0.2516 - Time: 489.42 sec\n",
      "✅ 모델이 개선되어 체크포인트 저장!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|███████████████████████████████████████| 3227/3227 [08:08<00:00,  6.61it/s, Loss=4.85, Accuracy=0.324]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 2/20 - Loss: 4.9178 - Accuracy: 0.3239 - Time: 488.12 sec\n",
      "✅ 모델이 개선되어 체크포인트 저장!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20:  69%|██████████████████████████▉            | 2226/3227 [05:37<02:30,  6.64it/s, Loss=5.06, Accuracy=0.372]"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW, get_scheduler\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from datetime import datetime\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# ✅ 2. 모델 생성 (train_df의 감정, 상황 레이블 수에 맞춰 설정)\n",
    "num_emotions = len(train_df[\"emotion\"].unique())  # 감정 클래스 개수\n",
    "num_situations = len(train_df[\"situation\"].unique())  # 상황 클래스 개수\n",
    "model_name = \"monologg/kobert\"\n",
    "\n",
    "model = KoBERTMultiOutput(model_name, num_emotions, num_situations)  # 모델 초기화\n",
    "\n",
    "# ✅ 3. 디바이스 설정 (GPU 또는 CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "model.to(device)  # 모델을 설정한 디바이스로 이동\n",
    "\n",
    "# ✅ 4. 클래스 가중치 계산\n",
    "emotion_labels = train_df['emotion'].values\n",
    "situation_labels = train_df['situation'].values\n",
    "\n",
    "# 감정과 상황에 대한 클래스 가중치 계산\n",
    "emotion_class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(emotion_labels), y=emotion_labels)\n",
    "situation_class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(situation_labels), y=situation_labels)\n",
    "\n",
    "# 가중치를 텐서로 변환\n",
    "emotion_weights = torch.tensor(emotion_class_weights, dtype=torch.float).to(device)\n",
    "situation_weights = torch.tensor(situation_class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# ✅ 5. 손실 함수 설정 (가중치 적용)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=emotion_weights)  # 감정 클래스 가중치 적용\n",
    "situation_loss_fn = nn.CrossEntropyLoss(weight=situation_weights)  # 상황 클래스 가중치 적용\n",
    "\n",
    "# ✅ 6. 옵티마이저 및 학습률 스케줄러 설정\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)  # AdamW 옵티마이저 사용\n",
    "num_training_steps = len(train_dataloader) * 20  # 학습할 총 스텝 수 \n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)  # 학습률 스케줄러 설정\n",
    "\n",
    "# ✅ 7. 모델 학습\n",
    "num_epochs = 20\n",
    "train_accuracy_history = []  # 정확도 기록\n",
    "train_loss_history = []  # 손실값 기록\n",
    "\n",
    "# 얼리스탑을 위한 변수 초기화\n",
    "best_accuracy = 0.0  # 가장 좋은 정확도\n",
    "patience = 5  # 개선되지 않아도 허용할 에포크 수\n",
    "early_stopping_counter = 0  # 얼리스탑 카운터\n",
    "\n",
    "# 모델 체크포인트 저장을 위한 경로 설정\n",
    "model_path = \"kobert_emotion_situation\"\n",
    "os.makedirs(model_path, exist_ok=True)  # 디렉토리가 없으면 생성\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ✅ 8. 학습 시작\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # 모델을 학습 모드로 설정\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    epoch_accuracy = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=True)\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        # 배치 데이터를 디바이스로 이동\n",
    "        input_ids, attention_mask, token_type_ids, emotion_labels, situation_labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        token_type_ids = token_type_ids.to(device)\n",
    "        emotion_labels = emotion_labels.to(device)\n",
    "        situation_labels = situation_labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        emotion_logits, situation_logits = model(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "        # 손실 계산 (가중치 적용)\n",
    "        emotion_loss = loss_fn(emotion_logits, emotion_labels)\n",
    "        situation_loss = situation_loss_fn(situation_logits, situation_labels)\n",
    "\n",
    "        loss = emotion_loss + situation_loss\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()  # 기울기 초기화\n",
    "        loss.backward()  # 기울기 계산\n",
    "        optimizer.step()  # 최적화\n",
    "        lr_scheduler.step()  # 학습률 스케줄러 업데이트\n",
    "\n",
    "        total_loss += loss.item()  # 손실값 누적\n",
    "\n",
    "        # 정확도 계산\n",
    "        emotion_pred = torch.argmax(emotion_logits, dim=1)\n",
    "        situation_pred = torch.argmax(situation_logits, dim=1)\n",
    "\n",
    "        correct += (emotion_pred == emotion_labels).sum().item()\n",
    "        correct += (situation_pred == situation_labels).sum().item()\n",
    "        total += emotion_labels.size(0) + situation_labels.size(0)\n",
    "\n",
    "        # 정확도 업데이트\n",
    "        epoch_accuracy = correct / total\n",
    "\n",
    "        # 진행상황을 바에 손실값과 정확도 표시\n",
    "        progress_bar.set_postfix({\"Loss\": loss.item(), \"Accuracy\": epoch_accuracy})\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)  # 평균 손실 계산\n",
    "    epoch_accuracy = correct / total  # 에포크별 정확도 계산\n",
    "\n",
    "    # 에포크별 손실과 정확도 기록\n",
    "    train_loss_history.append(avg_loss)\n",
    "    train_accuracy_history.append(epoch_accuracy)\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"✅ Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f} - Accuracy: {epoch_accuracy:.4f} - Time: {epoch_time:.2f} sec\")\n",
    "\n",
    "    # 얼리스탑 조건: 성능이 개선되지 않으면 학습을 중지\n",
    "    if epoch_accuracy > best_accuracy:\n",
    "        best_accuracy = epoch_accuracy\n",
    "        early_stopping_counter = 0\n",
    "        # 현재 모델을 체크포인트로 저장\n",
    "        torch.save(model.state_dict(), f\"{model_path}/best_model.pth\")\n",
    "        print(\"✅ 모델이 개선되어 체크포인트 저장!\")\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        print(f\"❌ 모델 성능이 개선되지 않았습니다. (이미 개선된 횟수: {early_stopping_counter})\")\n",
    "\n",
    "    # 얼리스탑 발동 여부 확인\n",
    "    if early_stopping_counter >= patience:\n",
    "        print(f\"⚠️ 얼리스탑 발동! 성능이 개선되지 않았습니다. (최대 {patience} 에포크)\")\n",
    "        break  # 학습 중지\n",
    "\n",
    "print(\"🎉 KoBERT 학습 완료!\")\n",
    "\n",
    "# ✅ 모델 저장 (최종 모델 가중치 및 학습 기록 포함)\n",
    "torch.save(model.state_dict(), f\"{model_path}/final_model.pth\")  # 최종 모델 저장\n",
    "torch.save({'train_loss': train_loss_history, 'train_accuracy': train_accuracy_history}, f\"{model_path}/history.pth\")  # 학습 기록\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15896cc2-2187-45bd-9ad2-232a01e69641",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
